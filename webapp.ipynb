{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e62ce05",
   "metadata": {},
   "source": [
    "# Neural Style Transfer Web Application\n",
    "\n",
    "This notebook creates a simple web interface for neural style transfer using Gradio. Users can:\n",
    "1. Upload a content image\n",
    "2. Upload a style image\n",
    "3. Select style intensity with a slider\n",
    "4. Generate a stylized image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752a3290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "251bde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model components from model.ipynb\n",
    "\n",
    "# Custom VGG model for style transfer\n",
    "class VGGStyleTransfer(nn.Module):\n",
    "    def __init__(self, fine_tune=False):\n",
    "        super(VGGStyleTransfer, self).__init__()\n",
    "        vgg = models.vgg19(weights='DEFAULT').features.eval().to(device)\n",
    "        \n",
    "        # Use multiple content layers for better structural representation\n",
    "        self.content_layers = ['conv4_2', 'conv5_2']\n",
    "        self.style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
    "        \n",
    "        self.layer_names = {\n",
    "            'conv1_1': 0, 'conv1_2': 2,\n",
    "            'conv2_1': 5, 'conv2_2': 7,\n",
    "            'conv3_1': 10, 'conv3_2': 12, 'conv3_3': 14, 'conv3_4': 16,\n",
    "            'conv4_1': 19, 'conv4_2': 21, 'conv4_3': 23, 'conv4_4': 25,\n",
    "            'conv5_1': 28, 'conv5_2': 30, 'conv5_3': 32, 'conv5_4': 34\n",
    "        }\n",
    "        \n",
    "        max_index = max(self.layer_names.values())\n",
    "        self.model = nn.Sequential()\n",
    "        for i, layer in enumerate(vgg):\n",
    "            if i <= max_index:\n",
    "                self.model.add_module(str(i), layer)\n",
    "        \n",
    "        # Fine-tune specific layers\n",
    "        if fine_tune:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if '19' in name or '21' in name:  # conv4_1, conv4_2\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        content_features = {}\n",
    "        style_features = {}\n",
    "        for name, layer in self.model.named_children():\n",
    "            x = layer(x)\n",
    "            layer_idx = int(name)\n",
    "            for layer_name, idx in self.layer_names.items():\n",
    "                if idx == layer_idx:\n",
    "                    if layer_name in self.content_layers:\n",
    "                        content_features[layer_name] = x.clone()\n",
    "                    if layer_name in self.style_layers:\n",
    "                        style_features[layer_name] = x.clone()\n",
    "        return content_features, style_features\n",
    "\n",
    "# Improved Gram matrix calculation\n",
    "def gram_matrix(feature):\n",
    "    batch_size, channels, height, width = feature.size()\n",
    "    # Normalize features to prevent extreme values\n",
    "    feature = feature / (feature.std() + 1e-8)\n",
    "    feature = feature.view(batch_size * channels, height * width)\n",
    "    gram = torch.mm(feature, feature.t())\n",
    "    # Normalize by size for consistent weighting across layers\n",
    "    return gram.div(batch_size * channels * height * width)\n",
    "\n",
    "# Enhanced loss functions\n",
    "class StyleTransferLoss:\n",
    "    def __init__(self, content_layers, style_layers):\n",
    "        self.content_layers = content_layers\n",
    "        self.style_layers = style_layers\n",
    "        # Weight different layers differently\n",
    "        self.style_weights = {\n",
    "            'conv1_1': 1.0,\n",
    "            'conv2_1': 0.8,\n",
    "            'conv3_1': 0.5,\n",
    "            'conv4_1': 0.3, \n",
    "            'conv5_1': 0.1\n",
    "        }\n",
    "        self.content_weights = {\n",
    "            'conv4_2': 1.0,\n",
    "            'conv5_2': 0.5\n",
    "        }\n",
    "    \n",
    "    def compute_loss(self, content_features, style_features, generated_features, threshold):\n",
    "        content_loss = 0\n",
    "        for layer in self.content_layers:\n",
    "            weight = self.content_weights.get(layer, 1.0)\n",
    "            content_loss += weight * nn.functional.mse_loss(\n",
    "                generated_features[0][layer], content_features[layer]\n",
    "            )\n",
    "        \n",
    "        style_loss = 0\n",
    "        for layer in self.style_layers:\n",
    "            weight = self.style_weights.get(layer, 1.0)\n",
    "            gen_gram = gram_matrix(generated_features[1][layer])\n",
    "            style_gram = gram_matrix(style_features[layer])\n",
    "            style_loss += weight * nn.functional.mse_loss(gen_gram, style_gram)\n",
    "        \n",
    "        # Improved dynamic weighting for better style vs content balance\n",
    "        # Use cubic function for smoother transition\n",
    "        content_weight = max(0.2, (1.0 - threshold) ** 3)\n",
    "        style_weight = max(0.2, threshold ** 2)\n",
    "        \n",
    "        # Adjust scales based on threshold for better balance\n",
    "        content_scale = 1e5 if threshold < 0.5 else 5e4\n",
    "        style_scale = 1e10 if threshold > 0.5 else 5e9\n",
    "        \n",
    "        content_loss *= content_weight * content_scale\n",
    "        style_loss *= style_weight * style_scale\n",
    "        \n",
    "        # Enhanced total variation loss with better weighting\n",
    "        tv_weight = 1e2 * (1.0 - threshold * 0.5)  # Reduce TV loss as style intensity increases\n",
    "        tv_loss = (\n",
    "            torch.sum(torch.abs(generated_features[2][:, :, :, :-1] - generated_features[2][:, :, :, 1:])) +\n",
    "            torch.sum(torch.abs(generated_features[2][:, :, :-1, :] - generated_features[2][:, :, 1:, :]))\n",
    "        ) * tv_weight\n",
    "        \n",
    "        # L2 regularization - reduced for better creativity\n",
    "        l2_reg = torch.norm(generated_features[2], p=2) * 5e-4\n",
    "        \n",
    "        total_loss = content_loss + style_loss + tv_loss + l2_reg\n",
    "        return total_loss, content_loss, style_loss, tv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14fb3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformation functions\n",
    "def get_transform(img_size=512):\n",
    "    \"\"\"Define image transformations for preprocessing\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.CenterCrop((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def preprocess_image(img, img_size=512, preserve_aspect=True):\n",
    "    \"\"\"Preprocess an image with optional aspect ratio preservation\"\"\"\n",
    "    if preserve_aspect:\n",
    "        # Calculate the new dimensions while preserving aspect ratio\n",
    "        width, height = img.size\n",
    "        if width > height:\n",
    "            new_width = img_size\n",
    "            new_height = int(height * img_size / width)\n",
    "        else:\n",
    "            new_height = img_size\n",
    "            new_width = int(width * img_size / height)\n",
    "        img = transforms.Resize((new_height, new_width))(img)\n",
    "        # Center pad to square\n",
    "        result = Image.new(img.mode, (img_size, img_size), (0, 0, 0))\n",
    "        offset_x = (img_size - new_width) // 2\n",
    "        offset_y = (img_size - new_height) // 2\n",
    "        result.paste(img, (offset_x, offset_y))\n",
    "        img = result\n",
    "    \n",
    "    # Apply standard transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Convert tensor to numpy image for display\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "    img = tensor * std + mean\n",
    "    img = img.clamp(0, 1).squeeze(0).detach().cpu().permute(1, 2, 0).numpy()\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "# Initialize the model (do it once to save time)\n",
    "model = VGGStyleTransfer(fine_tune=False).to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae341bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main style transfer function for the web interface\n",
    "def style_transfer(content_img, style_img, style_intensity=0.5, num_steps=100, img_size=512, progress=gr.Progress()):\n",
    "    \"\"\"\n",
    "    Apply style transfer to the content image using the style image.\n",
    "    \n",
    "    Args:\n",
    "        content_img: Content image (PIL Image)\n",
    "        style_img: Style image (PIL Image)\n",
    "        style_intensity: Style intensity (0.0 to 1.0)\n",
    "        num_steps: Number of optimization steps\n",
    "        img_size: Size of the output image\n",
    "        progress: Gradio progress bar\n",
    "    \n",
    "    Returns:\n",
    "        Stylized image (numpy array)\n",
    "    \"\"\"\n",
    "    if content_img is None or style_img is None:\n",
    "        raise ValueError(\"Both content and style images must be provided\")\n",
    "    \n",
    "    # Prepare images with enhanced preprocessing\n",
    "    content_tensor = preprocess_image(content_img, img_size=img_size, preserve_aspect=True)\n",
    "    style_tensor = preprocess_image(style_img, img_size=img_size, preserve_aspect=True)\n",
    "    \n",
    "    # Initialize the generated image with content image + small noise for better convergence\n",
    "    generated_img = content_tensor.clone()\n",
    "    noise = torch.randn_like(generated_img) * 0.03\n",
    "    generated_img = generated_img + noise\n",
    "    generated_img = torch.clamp(generated_img, 0, 1).requires_grad_(True)\n",
    "    \n",
    "    # Use advanced optimizer settings\n",
    "    optimizer = optim.LBFGS([generated_img], lr=0.1, max_iter=1)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "    loss_criterion = StyleTransferLoss(model.content_layers, model.style_layers)\n",
    "    \n",
    "    # Extract content and style features\n",
    "    content_features, _ = model(content_tensor)\n",
    "    _, style_features = model(style_tensor)\n",
    "    \n",
    "    # Track loss progress for early stopping\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # Style transfer optimization loop\n",
    "    progress(0, desc=\"Starting style transfer...\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Ensure pixel values are in valid range\n",
    "            with torch.no_grad():\n",
    "                generated_img.clamp_(0, 1)\n",
    "            \n",
    "            # Get features of generated image\n",
    "            gen_content_features, gen_style_features = model(generated_img)\n",
    "            gen_features = (gen_content_features, gen_style_features, generated_img)\n",
    "            \n",
    "            # Compute loss with enhanced weighting\n",
    "            total_loss, content_loss, style_loss, tv_loss = loss_criterion.compute_loss(\n",
    "                content_features, style_features, gen_features, style_intensity\n",
    "            )\n",
    "            \n",
    "            # Calculate loss stats for logging\n",
    "            stats = {\n",
    "                \"total\": total_loss.item(),\n",
    "                \"content\": content_loss.item(),\n",
    "                \"style\": style_loss.item(),\n",
    "                \"tv\": tv_loss.item()\n",
    "            }\n",
    "            \n",
    "            # Backpropagate\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Store stats for return\n",
    "            closure.stats = stats\n",
    "            closure.loss = total_loss.item()\n",
    "            return total_loss\n",
    "        \n",
    "        # Run optimization step\n",
    "        optimizer.step(closure)\n",
    "        current_loss = closure.loss\n",
    "        \n",
    "        # Update learning rate\n",
    "        if step % 20 == 0 and step > 0:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            no_improvement_count = 0\n",
    "            # Save the best image state\n",
    "            with torch.no_grad():\n",
    "                best_img = generated_img.clone()\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "        \n",
    "        # Early stopping if no improvement after 30 steps\n",
    "        if no_improvement_count > 30 and step > 50:\n",
    "            print(\"Early stopping due to no improvement\")\n",
    "            generated_img.data = best_img.data\n",
    "            break\n",
    "        \n",
    "        # Update progress\n",
    "        progress((step + 1) / num_steps, \n",
    "                desc=f\"Step {step+1}/{num_steps}: Loss = {closure.stats['total']:.2f} \" +\n",
    "                      f\"(Content: {closure.stats['content']:.2f}, Style: {closure.stats['style']:.2f})\")\n",
    "    \n",
    "    # Ensure final image is in valid range and use the best result\n",
    "    with torch.no_grad():\n",
    "        generated_img.clamp_(0, 1)\n",
    "    \n",
    "    # Convert to numpy for display\n",
    "    stylized_img = denormalize(generated_img)\n",
    "    progress(1.0, desc=\"Style transfer complete!\")\n",
    "    \n",
    "    # Save the result\n",
    "    timestamp = int(time.time())\n",
    "    output_path = os.path.join(\"output\", f\"stylized_{timestamp}.jpg\")\n",
    "    Image.fromarray(stylized_img).save(output_path)\n",
    "    \n",
    "    return stylized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6be965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gradio interface\n",
    "def create_interface():\n",
    "    with gr.Blocks(title=\"Neural Style Transfer\") as interface:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # Enhanced Neural Style Transfer App\n",
    "        \n",
    "        Upload a content image and a style image, then adjust the style intensity to create your stylized image!\n",
    "        \n",
    "        - Style Intensity: 0.0 = mostly content, 1.0 = mostly style\n",
    "        - Steps: More steps give better results but take longer (200-500 recommended for best results)\n",
    "        - Image Size: Larger sizes preserve more details but take longer to process\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                content_input = gr.Image(label=\"Content Image\", type=\"pil\")\n",
    "                style_input = gr.Image(label=\"Style Image\", type=\"pil\")\n",
    "            \n",
    "                with gr.Row():\n",
    "                    style_intensity = gr.Slider(\n",
    "                        minimum=0.0, maximum=1.0, value=0.7, step=0.05,\n",
    "                        label=\"Style Intensity\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    steps = gr.Slider(\n",
    "                        minimum=50, maximum=500, value=200, step=50,\n",
    "                        label=\"Steps (Iterations)\"\n",
    "                    )\n",
    "                    \n",
    "                    img_size = gr.Slider(\n",
    "                        minimum=256, maximum=1024, value=512, step=64,\n",
    "                        label=\"Image Size (pixels)\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    examples = gr.Examples(\n",
    "                        examples=[\n",
    "                            [\"Content/test/0016.jpg\", \"style/test/1.jpg\"],\n",
    "                            [\"Content/test/0771.jpg\", \"style/test/2.jpg\"]\n",
    "                        ],\n",
    "                        inputs=[content_input, style_input],\n",
    "                        label=\"Example Images\"\n",
    "                    )\n",
    "                \n",
    "                transfer_btn = gr.Button(\"Generate Stylized Image\", variant=\"primary\")\n",
    "            \n",
    "            with gr.Column(scale=1):\n",
    "                output_image = gr.Image(label=\"Stylized Image\")\n",
    "                info_text = gr.Markdown(\"\"\"\n",
    "                ### Tips for better results:\n",
    "                \n",
    "                1. For **detailed paintings**, use style intensity 0.6-0.8 and at least 200 steps\n",
    "                2. For **abstract patterns**, use style intensity 0.8-1.0\n",
    "                3. For **subtle effects**, use style intensity 0.3-0.5\n",
    "                4. **Higher resolution** images (512-1024px) produce better details but take longer\n",
    "                5. If the result looks poor, try a different style image or adjust the intensity\n",
    "                \"\"\")\n",
    "        \n",
    "        transfer_btn.click(\n",
    "            fn=style_transfer,\n",
    "            inputs=[content_input, style_input, style_intensity, steps, img_size],\n",
    "            outputs=output_image\n",
    "        )\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Launch the interface\n",
    "demo = create_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7da7629b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://d68668b77a534787c1.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "* Running on public URL: https://d68668b77a534787c1.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d68668b77a534787c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch the interface with sharing enabled\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8de04a",
   "metadata": {},
   "source": [
    "## Instructions for Using the Web App\n",
    "\n",
    "1. **Upload a Content Image**: This is the base image that will be stylized.\n",
    "2. **Upload a Style Image**: This image provides the artistic style to apply.\n",
    "3. **Adjust the Style Intensity**: Lower values preserve more of the content image details, while higher values apply more of the style image's artistic elements.\n",
    "4. **Set the Number of Steps**: More steps typically yield better results but increase processing time. 200-300 steps is recommended for quality results.\n",
    "5. **Select Image Size**: Higher resolutions (512-1024px) preserve more details but increase processing time.\n",
    "6. **Click \"Generate Stylized Image\"**: Wait for the process to complete (progress will be shown).\n",
    "\n",
    "## Technical Notes\n",
    "\n",
    "- The app uses a VGG19 model pre-trained on ImageNet for feature extraction.\n",
    "- Style transfer is performed through an optimization process with an advanced loss function that balances content preservation and style adaptation.\n",
    "- Content representation now uses both `conv4_2` and `conv5_2` layers for better structural detail.\n",
    "- Style representation uses layers `conv1_1`, `conv2_1`, `conv3_1`, `conv4_1`, `conv5_1` with decreasing weights for multi-scale style features.\n",
    "- The optimization uses the L-BFGS optimizer with dynamic learning rate scheduling for better convergence.\n",
    "- Total variation loss is applied to ensure spatial smoothness in the generated image.\n",
    "- Early stopping is implemented to prevent over-optimization once quality plateaus.\n",
    "- Images are preprocessed with proper aspect ratio preservation to prevent distortion.\n",
    "\n",
    "## Troubleshooting Poor Results\n",
    "\n",
    "- **Problem**: Stylized image looks too much like original content\n",
    "  - **Solution**: Increase style intensity (0.7-0.9) and use more steps\n",
    "\n",
    "- **Problem**: Style details don't appear clearly\n",
    "  - **Solution**: Try a higher image resolution (512-1024) and more steps (300-500)\n",
    "\n",
    "- **Problem**: Output appears blocky or distorted\n",
    "  - **Solution**: Ensure your style image has clear, distinctive features and isn't too small\n",
    "\n",
    "- **Problem**: Colors look washed out\n",
    "  - **Solution**: Style images with vibrant colors work best; try a different style image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
