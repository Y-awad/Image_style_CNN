{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b609898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cd360",
   "metadata": {},
   "source": [
    "# Data loading and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880e9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing splits found. Skipping sampling and splitting.\n",
      "Using images in Content/train/, Content/validation/, Content/test/, style/train/, style/validation/, style/test/.\n",
      "Final folder structure:\n",
      "Content train images: 4876\n",
      "Content validation images: 1049\n",
      "Content test images: 1050\n",
      "Style train images: 2100\n",
      "Style validation images: 450\n",
      "Style test images: 450\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Define paths\n",
    "base_dir = \".\"\n",
    "content_base_dir = os.path.join(base_dir, \"Content\", \"images\", \"images\")\n",
    "style_dir = os.path.join(base_dir, \"style\")\n",
    "\n",
    "# Output directories for splits\n",
    "content_train_dir = os.path.join(base_dir, \"Content\", \"train\")\n",
    "content_val_dir = os.path.join(base_dir, \"Content\", \"validation\")\n",
    "content_test_dir = os.path.join(base_dir, \"Content\", \"test\")\n",
    "style_train_dir = os.path.join(base_dir, \"style\", \"train\")\n",
    "style_val_dir = os.path.join(base_dir, \"style\", \"validation\")\n",
    "style_test_dir = os.path.join(base_dir, \"style\", \"test\")\n",
    "\n",
    "# Check if split directories already contain images\n",
    "def check_existing_splits(dirs):\n",
    "    for directory in dirs:\n",
    "        if os.path.exists(directory) and len(os.listdir(directory)) > 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "split_dirs = [content_train_dir, content_val_dir, content_test_dir, \n",
    "              style_train_dir, style_val_dir, style_test_dir]\n",
    "\n",
    "if check_existing_splits(split_dirs):\n",
    "    print(\"Existing splits found. Skipping sampling and splitting.\")\n",
    "    print(\"Using images in Content/train/, Content/validation/, Content/test/, style/train/, style/validation/, style/test/.\")\n",
    "else:\n",
    "    # Check if input directories exist and contain images\n",
    "    def check_input_directories(content_dir, style_dir):\n",
    "        if not os.path.exists(content_dir) or not any(os.path.isdir(os.path.join(content_dir, d)) for d in os.listdir(content_dir)):\n",
    "            raise FileNotFoundError(f\"Content directory {content_dir} is missing or empty. Please populate it with images in subfolders (e.g., architecture, art and culture).\")\n",
    "        if not os.path.exists(style_dir) or not any(f.lower().endswith((\".jpg\", \".jpeg\", \".png\")) for f in os.listdir(style_dir)):\n",
    "            raise FileNotFoundError(f\"Style directory {style_dir} is missing or empty. Please populate it with style images.\")\n",
    "\n",
    "    # Helper function to get image paths from subfolders\n",
    "    def get_image_paths(base_directory):\n",
    "        image_paths = defaultdict(list)\n",
    "        for category in os.listdir(base_directory):\n",
    "            category_path = os.path.join(base_directory, category)\n",
    "            if os.path.isdir(category_path):\n",
    "                image_paths[category] = [os.path.join(category_path, f) for f in os.listdir(category_path)\n",
    "                                        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "        return image_paths\n",
    "\n",
    "    # Check input directories\n",
    "    check_input_directories(content_base_dir, style_dir)\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    for directory in [content_train_dir, content_val_dir, content_test_dir, \n",
    "                      style_train_dir, style_val_dir, style_test_dir]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Load all image paths\n",
    "    content_images = get_image_paths(content_base_dir)\n",
    "    style_images = [os.path.join(style_dir, f) for f in os.listdir(style_dir) \n",
    "                    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    # Print initial counts\n",
    "    total_content = sum(len(images) for images in content_images.values())\n",
    "    print(f\"Total content images: {total_content}\")\n",
    "    print(f\"Total style images: {len(style_images)}\")\n",
    "\n",
    "    # Step 1: Sample the datasets\n",
    "    # Target: 7,000 content images, 3,000 style images\n",
    "    target_content = 7000\n",
    "    target_style = 3000\n",
    "\n",
    "    # Sample content proportionally from each category\n",
    "    content_sampled = []\n",
    "    total_images = sum(len(images) for images in content_images.values())\n",
    "    for category, images in content_images.items():\n",
    "        if total_images > 0:\n",
    "            category_target = int((len(images) / total_images) * target_content)\n",
    "            category_target = min(category_target, len(images))  # Don't exceed available images\n",
    "            sampled = random.sample(images, category_target) if len(images) >= category_target else images\n",
    "            content_sampled.extend(sampled)\n",
    "\n",
    "    # Adjust if still over 7,000 due to rounding\n",
    "    if len(content_sampled) > target_content:\n",
    "        content_sampled = random.sample(content_sampled, target_content)\n",
    "    elif len(content_sampled) < target_content and total_images > len(content_sampled):\n",
    "        additional = random.sample([img for cat in content_images.values() for img in cat \n",
    "                                  if img not in content_sampled], target_content - len(content_sampled))\n",
    "        content_sampled.extend(additional)\n",
    "\n",
    "    # Sample style images\n",
    "    style_sampled = random.sample(style_images, target_style) if len(style_images) >= target_style else style_images\n",
    "\n",
    "    print(f\"Sampled content images: {len(content_sampled)}\")\n",
    "    print(f\"Sampled style images: {len(style_sampled)}\")\n",
    "\n",
    "    # Step 2: Split the data (70/15/15)\n",
    "    # Content split\n",
    "    content_train, content_temp = train_test_split(content_sampled, train_size=0.7, random_state=42)\n",
    "    content_val, content_test = train_test_split(content_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Style split\n",
    "    style_train, style_temp = train_test_split(style_sampled, train_size=0.7, random_state=42)\n",
    "    style_val, style_test = train_test_split(style_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    print(f\"Content split - Train: {len(content_train)}, Val: {len(content_val)}, Test: {len(content_test)}\")\n",
    "    print(f\"Style split - Train: {len(style_train)}, Val: {len(style_val)}, Test: {len(style_test)}\")\n",
    "\n",
    "    # Step 3: Move images to respective split directories\n",
    "    def move_images(image_paths, destination_dir):\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                shutil.move(img_path, os.path.join(destination_dir, os.path.basename(img_path)))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to move {img_path}: {e}\")\n",
    "\n",
    "    # Move content images\n",
    "    move_images(content_train, content_train_dir)\n",
    "    move_images(content_val, content_val_dir)\n",
    "    move_images(content_test, content_test_dir)\n",
    "\n",
    "    # Move style images\n",
    "    move_images(style_train, style_train_dir)\n",
    "    move_images(style_val, style_val_dir)\n",
    "    move_images(style_test, style_test_dir)\n",
    "\n",
    "    # Step 4: Verify splits before deleting remaining images\n",
    "    print(\"Verifying splits before deleting remaining images...\")\n",
    "    print(f\"Content train images: {len(os.listdir(content_train_dir))}\")\n",
    "    print(f\"Content validation images: {len(os.listdir(content_val_dir))}\")\n",
    "    print(f\"Content test images: {len(os.listdir(content_test_dir))}\")\n",
    "    print(f\"Style train images: {len(os.listdir(style_train_dir))}\")\n",
    "    print(f\"Style validation images: {len(os.listdir(style_val_dir))}\")\n",
    "    print(f\"Style test images: {len(os.listdir(style_test_dir))}\")\n",
    "\n",
    "    # Step 5: Delete remaining (unused) images in original folders\n",
    "    # Reload remaining content images\n",
    "    remaining_content_images = get_image_paths(content_base_dir)\n",
    "    remaining_style_images = [os.path.join(style_dir, f) for f in os.listdir(style_dir) \n",
    "                             if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    # Delete remaining content images\n",
    "    print(\"Deleting remaining content images...\")\n",
    "    for category, images in remaining_content_images.items():\n",
    "        category_path = os.path.join(content_base_dir, category)\n",
    "        for img_path in images:\n",
    "            try:\n",
    "                os.remove(img_path)\n",
    "                print(f\"Deleted {img_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {img_path}: {e}\")\n",
    "        # Remove category folder if empty\n",
    "        if not os.listdir(category_path):\n",
    "            shutil.rmtree(category_path)\n",
    "            print(f\"Removed empty category folder: {category_path}\")\n",
    "\n",
    "    # Delete remaining style images\n",
    "    print(\"Deleting remaining style images...\")\n",
    "    for img_path in remaining_style_images:\n",
    "        try:\n",
    "            os.remove(img_path)\n",
    "            print(f\"Deleted {img_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {img_path}: {e}\")\n",
    "\n",
    "# Final verification (always executed)\n",
    "print(\"Final folder structure:\")\n",
    "print(f\"Content train images: {len(os.listdir(content_train_dir))}\")\n",
    "print(f\"Content validation images: {len(os.listdir(content_val_dir))}\")\n",
    "print(f\"Content test images: {len(os.listdir(content_test_dir))}\")\n",
    "print(f\"Style train images: {len(os.listdir(style_train_dir))}\")\n",
    "print(f\"Style validation images: {len(os.listdir(style_val_dir))}\")\n",
    "print(f\"Style test images: {len(os.listdir(style_test_dir))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba2286",
   "metadata": {},
   "source": [
    "# Image Preprocessing and Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd48da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "\n",
    "# Define image transformations for preprocessing\n",
    "def get_transforms(img_size=256):\n",
    "    # Transformations for both content and style images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "# Custom dataset for neural style transfer\n",
    "class StyleTransferDataset(Dataset):\n",
    "    def __init__(self, content_dir, style_dir, transform=None, threshold_values=None, max_samples=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            content_dir: Directory with content images\n",
    "            style_dir: Directory with style images\n",
    "            transform: Optional transform to be applied on images\n",
    "            threshold_values: List of threshold values for style transfer intensity\n",
    "            max_samples: Maximum number of content-style pairs to use (None = use all combinations)\n",
    "        \"\"\"\n",
    "        self.content_paths = [os.path.join(content_dir, f) for f in os.listdir(content_dir) \n",
    "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.style_paths = [os.path.join(style_dir, f) for f in os.listdir(style_dir) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Default threshold values if none provided\n",
    "        if threshold_values is None:\n",
    "            # Values between 0.0 (content-focused) and 1.0 (style-focused)\n",
    "            self.threshold_values = [0.2, 0.4, 0.6, 0.8]\n",
    "        else:\n",
    "            self.threshold_values = threshold_values\n",
    "            \n",
    "        # Create pairs of content-style indices\n",
    "        content_indices = range(len(self.content_paths))\n",
    "        style_indices = range(len(self.style_paths))\n",
    "        \n",
    "        # Generate all possible combinations\n",
    "        all_combinations = list(itertools.product(content_indices, style_indices))\n",
    "        \n",
    "        # Sample if max_samples is specified\n",
    "        if max_samples and max_samples < len(all_combinations):\n",
    "            # Shuffle and take max_samples\n",
    "            random.seed(42)  # For reproducibility\n",
    "            random.shuffle(all_combinations)\n",
    "            self.combinations = all_combinations[:max_samples]\n",
    "        else:\n",
    "            self.combinations = all_combinations\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.combinations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get content and style indices from precomputed combinations\n",
    "        content_idx, style_idx = self.combinations[idx]\n",
    "        \n",
    "        # Load content and style images\n",
    "        content_img = Image.open(self.content_paths[content_idx]).convert('RGB')\n",
    "        style_img = Image.open(self.style_paths[style_idx]).convert('RGB')\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            content_img = self.transform(content_img)\n",
    "            style_img = self.transform(style_img)\n",
    "        \n",
    "        # Randomly select a threshold value\n",
    "        threshold = random.choice(self.threshold_values)\n",
    "        \n",
    "        # Return a triplet: content image, style image, and threshold\n",
    "        return {\n",
    "            'content': content_img, \n",
    "            'style': style_img, \n",
    "            'threshold': torch.tensor(threshold, dtype=torch.float32),\n",
    "            'content_path': self.content_paths[content_idx],\n",
    "            'style_path': self.style_paths[style_idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "babe2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training, validation, and testing\n",
    "def create_dataloaders(batch_size=16, num_workers=4, img_size=256, threshold_values=None, \n",
    "                      max_train_samples=5000, max_val_samples=1000, max_test_samples=500):\n",
    "    # Define image transformations\n",
    "    transform = get_transforms(img_size=img_size)\n",
    "    \n",
    "    # Define paths to data directories\n",
    "    content_train_dir = os.path.join(\"Content\", \"train\")\n",
    "    content_val_dir = os.path.join(\"Content\", \"validation\")\n",
    "    content_test_dir = os.path.join(\"Content\", \"test\")\n",
    "    \n",
    "    style_train_dir = os.path.join(\"style\", \"train\")\n",
    "    style_val_dir = os.path.join(\"style\", \"validation\")\n",
    "    style_test_dir = os.path.join(\"style\", \"test\")\n",
    "    \n",
    "    # Create datasets with sample limits\n",
    "    train_dataset = StyleTransferDataset(content_train_dir, style_train_dir, transform, \n",
    "                                        threshold_values, max_samples=max_train_samples)\n",
    "    val_dataset = StyleTransferDataset(content_val_dir, style_val_dir, transform, \n",
    "                                      threshold_values, max_samples=max_val_samples)\n",
    "    test_dataset = StyleTransferDataset(content_test_dir, style_test_dir, transform, \n",
    "                                       threshold_values, max_samples=max_test_samples)\n",
    "    \n",
    "    # Print dataset details\n",
    "    print(f\"Train dataset: Using {len(train_dataset)} samples out of {len(os.listdir(content_train_dir)) * len(os.listdir(style_train_dir))} possible combinations\")\n",
    "    print(f\"Validation dataset: Using {len(val_dataset)} samples out of {len(os.listdir(content_val_dir)) * len(os.listdir(style_val_dir))} possible combinations\")\n",
    "    print(f\"Test dataset: Using {len(test_dataset)} samples out of {len(os.listdir(content_test_dir)) * len(os.listdir(style_test_dir))} possible combinations\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: Using 8000 samples out of 10239600 possible combinations\n",
      "Validation dataset: Using 1000 samples out of 472050 possible combinations\n",
      "Test dataset: Using 100 samples out of 472500 possible combinations\n",
      "Training samples: 8000\n",
      "Validation samples: 1000\n",
      "Test samples: 100\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Verify the dataset and data loaders\n",
    "# Note: Only run this after data preparation is complete\n",
    "\n",
    "# Create data loaders with specified batch size and sample limits\n",
    "batch_size = 8  # Smaller batch size for testing\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    batch_size=batch_size, \n",
    "    num_workers=2,\n",
    "    max_train_samples=7000,  # Limit train samples to 5000\n",
    "    max_val_samples=1000,    # Limit validation samples to 1000\n",
    "    max_test_samples=1000     # Limit test samples to 500\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Check a single batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shape for content images: {sample_batch['content'].shape}\")\n",
    "print(f\"Batch shape for style images: {sample_batch['style'].shape}\")\n",
    "print(f\"Threshold values: {sample_batch['threshold']}\")\n",
    "\n",
    "# Function to denormalize and display images\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# Display first few images in the batch (optional - uncomment to visualize)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(min(4, batch_size)):\n",
    "#     # Display content image\n",
    "     plt.subplot(2, 4, i + 1)\n",
    "     img = denormalize(sample_batch['content'][i:i+1]).squeeze(0).permute(1, 2, 0).numpy()\n",
    "     img = np.clip(img, 0, 1)\n",
    "     plt.imshow(img)\n",
    "     plt.title(f\"Content {i}\")\n",
    "     plt.axis('off')\n",
    "     \n",
    "#     # Display style image\n",
    "     plt.subplot(2, 4, i + 5)\n",
    "     img = denormalize(sample_batch['style'][i:i+1]).squeeze(0).permute(1, 2, 0).numpy()\n",
    "     img = np.clip(img, 0, 1)\n",
    "     plt.imshow(img)\n",
    "     plt.title(f\"Style {i} (t={sample_batch['threshold'][i]:.2f})\")\n",
    "     plt.axis('off')\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad728ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
